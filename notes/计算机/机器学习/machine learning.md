# 机器学习的基本概念

### 一、**数据相关术语**

|术语|含义|
|---|---|
|**特征（Feature）**|描述数据对象某个方面的属性，如图片的像素、人的身高等。|
|**样本（Sample）**|一条具体的数据记录，包含多个特征，例如一张照片或一个人的体检记录。|
|**标签（Label）**|样本对应的结果或类别（在监督学习中使用），比如“猫”或“狗”。|
|**数据集（Dataset）**|一组样本的集合，通常会分为训练集（Training Set）和测试集（Test Set）。|

### 二、**模型与学习相关术语**

|术语|含义|
|---|---|
|**模型（Model）**|学习到的规则或函数，用于对新数据进行预测。|
|**训练（Training）**|用已有的带标签数据让模型学习规律的过程。|
|**预测（Prediction）**|使用训练好的模型对新样本进行判断或估计。|
|**监督学习（Supervised Learning）**|输入数据和输出标签都是已知的，目标是学习输入到输出的映射。|
|**无监督学习（Unsupervised Learning）**|只有输入数据，没有标签，目标是发现数据中的结构（如聚类）。|
|**强化学习（Reinforcement Learning）**|通过与环境互动，学习最大化长期奖励的策略。|

### 三、**评估相关术语**

|术语|含义|
|---|---|
|**损失函数（Loss Function）**|衡量模型预测值与真实值之间差距的函数。|
|**准确率（Accuracy）**|预测正确的样本数与总样本数之比。|
|**过拟合（Overfitting）**|模型在训练集上表现很好，但在测试集上表现较差，泛化能力差。|
|**欠拟合（Underfitting）**|模型过于简单，无法很好地拟合训练数据。|

### 四、**优化相关术语**

|术语|含义|
|---|---|
|**梯度下降（Gradient Descent）**|一种优化算法，通过不断调整参数来最小化损失函数。|
|**学习率（Learning Rate）**|控制每次参数更新的幅度，过大或过小都会影响训练效果。|
|**正则化（Regularization）**|防止过拟合的技术，常见方法如L1和L2正则。|

### 五、**假设空间**

定义：假设空间是所有可能模型或函数的集合，机器学习算法会在其中寻找最优的假设来拟合训练数据。

|项目|内容|
|---|---|
|**本质**|一个函数集合，表示模型能学习的所有可能映射关系|
|**符号表示**|通常记作 **H**，例如：H={h∣h:X→Y}H = \{ h \mid h: X \rightarrow Y \}|
|**与模型关系**|模型训练就是从 H 中选择一个最优假设 h∗∈Hh^* \in H|
|**影响因素**|假设空间由所选模型类型、模型结构和参数决定|
|**大小影响**|太小 → 欠拟合；太大 → 过拟合|
|**不同算法的假设空间**|决策树：所有树形分类规则；线性模型：所有线性函数等|

---
# 模型评估与选择

### 1. 经验误差与过拟合

- **训练误差（经验误差）**：模型在训练集上的误差
    
- **测试误差**：模型在测试集上的误差
    
- **泛化误差**：模型在所有潜在样本上的期望误差（关键评估目标）
    

#### 🔺 过拟合（Overfitting）：

模型学得过于复杂，把训练集的噪声也学进来了 → 测试误差大  
**应对策略**：正则化、提前停止、减少模型复杂度等

#### 🔻 欠拟合（Underfitting）：

模型太简单，无法学到数据的规律 → 训练误差都很大  
**应对策略**：增强模型复杂度、增加训练轮数等

---

### 2. 评估方法

#### ✅ 留出法（Hold-out）

- 将数据随机分为训练集和测试集（如 3:1）
    
- 多次随机划分后取平均值可提高稳定性
    

#### ✅ 交叉验证法（Cross-Validation）

- 常见 **k折交叉验证**，如10折（k=10）
    
- **留一法（LOOCV）**：每次留一个样本作测试，计算开销大但结果稳
    

#### ✅ 自助法（Bootstrap）

- 从数据集中**有放回地采样**生成训练集
    
- 大约1/3的样本未被选中，可作为测试集
    
- 适合小数据集，但会改变数据分布
    

#### ✅ 模型选择与调参

- 拆分出验证集进行参数调优
    
- 最终模型应在全体训练数据上重训练
    

---

### 3. 性能度量

#### 📌 分类任务

- **准确率** = 正确分类数 / 总数
    
- **错误率** = 错误分类数 / 总数
    
- **查准率（Precision）** = TP / (TP + FP)
    
- **查全率（Recall）** = TP / (TP + FN)
    
- **F1分数**：调和平均，平衡查准率和查全率
    
- **ROC曲线 & AUC**：评估分类器排序能力（AUC越大越好）
    
- **代价敏感错误率**：考虑不同错误的代价差异
    

#### 📌 回归任务

- **均方误差（MSE）**
    
- **平均绝对误差（MAE）**
    
- **R²（决定系数）**
    

---

### 4. 比较检验

#### ✅ 为什么需要比较检验？

- 测试性能并不等于泛化性能
    
- 模型具有**随机性**，简单比较均值不可靠
    

#### 📌 方法总结：

- **二项检验**：适合判断一个模型是否达到某个泛化错误率
    
- **t检验**：多个测试误差的显著性分析（单个模型或两个模型比较）
    
- **成对t检验（交叉验证）**：比较两个模型在多个数据划分下的表现
    
- **5×2折交叉验证**：用于缓解交叉验证结果不独立的问题，增强可信度
    

---

### 5. 偏差与方差

#### ✅ 偏差（Bias）

- 模型预测与真实值的差异，反映拟合能力
    
- 偏差大 → 欠拟合
    

#### ✅ 方差（Variance）

- 同样大小训练集变动导致的性能变化，反映模型的稳定性
    
- 方差大 → 过拟合
    

#### ✅ 噪声（Noise）

- 数据本身的不确定性，是泛化误差的下界
    

📌 **泛化误差 = 偏差² + 方差 + 噪声**

---

